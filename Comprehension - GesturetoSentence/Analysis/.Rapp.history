1+2
whatever
whatever <- c(1,2,3,4)#
#
whatever
whatever
whatever <- c(2,3,4,5,'werer')
whatever
foo = c(3,12,4,7,6,1)
dim(foo)= c(2,3)
foo
dim(foo)= c(3,2)
foo
fisher.test(foo)
foo = c(1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1)
length(foo)
foo = c(1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,2, 1,1,1,1,0,2,1)
foo = c(1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1)
goo = c(1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,2, 1,1,1,1,0,2,1)
length(goo)
length(foo)
goo = c(1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,2, 1,1,1,1,0,2)
length(goo)
mean(goo)
foo = c(1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,2,2,2)
mean(foo)
foo = c(1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,2,2,2,2)
mean(foo)
foo = c(1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,2,2,2,0,0,2,2,2,2)
mean(foo)
foo = c(1,1,1,1,1,1,1,1,1,1,1,1,1,1,2,2,2,2,0,0,2,2,2,2)
mean(foo)
wilcox.test(foo, goo)
foo = c(1,1,1,1,1,1,1,1,1,1,1,1,1,2,2,2,2,2,0,0,2,2,2,2)
wilcox.test(foo, goo)
foo = c(1,1,1,1,1,1,1,1,1,1,1,1,2,2,2,2,2,2,0,0,2,2,2,2)
wilcox.test(foo, goo)
mean(foo)
foo = c(1,1,1,1,1,1,1,1,1,1,1,1,2,2,2,2,2,2,0,0,2,2,2,2)
goo = c(1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,2, 1,1,1,1,0,2)
mean(goo)
mean(foo)
wilcox.test(foo,goo, paired=FALSE)
real <- c(3,4,3,3,0,3,3,2,2,4,3)
unreal <- c(2,2,1,2,1,2,3,1,1,3,1)
length(real)
wilcox.test(real, unreal, paired=TRUE)
ls()
summary(twocondition_maximal_model)
twocondition_maximal_model
anova(twocondition_maximal_model, twocondition_nointer_model)
ls()
Preliminary data read-in & cleaning/reshaping.  Note below where interesting stuff starts.  Look for#
# 'STEVE START HERE'#
#
setwd("/Users/mekline/Dropbox/_Projects/NRSA/Adult Studies/ArcReach/Analysis Predict")#
library(languageR)#
library(stringr)#
library(lme4)#
library(multcomp)#
library(binom)#
mean.na.rm <- function(x) { mean(x,na.rm=T) }#
stderr <- function(x) sqrt(var(x)/length(x))#
#
stderr_binom <-function(x){#
	p = sum(x)/length(x)#
	q = 1-p#
	sqrt(p*q/length(x))#
}#
#
confint <- function(x){#
	qt(0.975,df=length(x)-1)*stderr(x)#
}#
#
confint_norm_lower <- function(x){#
	foo <- binom.confint(sum(x), length(x), conf.level = 0.95, methods = "asymptotic")#
	foo[5]#
#
}#
#
confint_norm_upper <- function(x){#
	foo <- binom.confint(sum(x), length(x), conf.level = 0.95, methods = "asymptotic")#
	foo[6]#
#
}#
#
turk.files <- list.files('batch')#
willow.files <- list.files('log')#
################################################################
## Read in the data files from turk#
list.number <- 1;  # keep track of the list number#
turkdata <- data.frame(NULL)#
for(f in turk.files) {#
	cat(f)#
	tmp <- read.csv(paste('batch/',f, sep=''), header=T)#
	tmp$turkfile <- f#
	turkdata <- rbind(turkdata, tmp)#
#
	list.number <- list.number + 1#
}#
#
dropped.columns <- c("HITId", "HITTypeId","Title", "Description", "Keywords", "Reward", "CreationTime", "MaxAssignments", 	"RequesterAnnotation", "AssignmentDurationInSeconds", "AutoApprovalDelayInSeconds", "Expiration", "NumberOfSimilarHITs", 	"LifetimeInSeconds", "AssignmentId", "AcceptTime", "AutoApprovalTime", "ApprovalTime", "RejectionTime", 	"RequesterFeedback", "AssignmentStatus","LifetimeApprovalRate", "Last30DaysApprovalRate","Last7DaysApprovalRate" )#
#
turkdata <- turkdata[, setdiff(names(turkdata), dropped.columns)]#
#
################################################################
## Read in the data files from willow#
willowdata <- data.frame(NULL)#
for (w in willow.files) {#
	cat(w)#
	tmp <- read.csv(paste('log/',w, sep=''), header=T)#
	tmp$willowfile <- w#
	tmp$willowcode <- paste(unlist(str_extract_all(w,"[0-9+]")),collapse='')#
	willowdata <- rbind(willowdata, tmp)#
}#
#
#Remove extra header lines...#
willowdata <- willowdata[willowdata$Paycode != "Paycode",]#
################################################################
## Merge the information from turk and willow!  #
#
#How many do we start with?#
length(unique(willowdata$Paycode)) #107#
length(unique(turkdata$WorkerId)) #53#
#Save and standardize#
turkdata$Paycode <- turkdata$Answer.payCode#
turkdata$Answer.payCode <- NA#
turkdata <- turkdata[,setdiff(names(turkdata), c("Answer.payCode"))]#
turkdata$HasTurk <- TRUE#
willowdata$HasWillow <- TRUE#
#
mydata <- NULL#
mydata <- merge(willowdata, turkdata, by=c("Paycode"), all.x=TRUE, all.y=TRUE)#
#
#If your line doesn't have Willow, you are a useless unmatched Turk entry!#
#
#Check who these jokers are!  Will want to make double sure they have not taken the survey twice...#
mydata[is.na(mydata$HasWillow),]$Paycode#
mydata[is.na(mydata$HasWillow),]$WorkerId#
##################################################################
## Check through the data for compliance and multiple takers (the general full-set solution!! #
## Use this for actually running the analysis!#
#
#If your line doesn't have Turk, we'll give you a free pass on language, country, and video presentation#
mydata[is.na(mydata$HasTurk),]$Answer.English <- "yes"#
mydata[is.na(mydata$HasTurk),]$Answer.country <- "USA"#
mydata[is.na(mydata$HasTurk),]$VideoProblem <- FALSE#
#
#Come up with a true subject labeling order#
mydata$willowSubNo <- as.numeric(as.character(mydata$willowSubNo))#
trueSub <- unique(mydata[,c('willowcode','willowSubNo', 'Paycode')])#
trueSub <- trueSub[order(trueSub$willowcode, trueSub$willowSubNo),]#
trueSub$trueSubNo <- 1:nrow(trueSub)#
#
#And add it back to the dataframe#
mydata <- merge(mydata, trueSub, by=c('willowcode','willowSubNo', 'Paycode'))#
#
#Now check if any Turk workerIDs have MORE THAN ONE subno#
participantNos <- aggregate(mydata$trueSubNo, by=list(mydata$WorkerId, mydata$trueSubNo), unique)#
participantNos <- participantNos[,1:2]#
names(participantNos) <- c('WorkerId','trueSubNo')#
#
#Make sure it's in the correct order!!#
participantNos <- participantNos[order(participantNos$trueSubNo),]#
participantNos$isDup <- duplicated(participantNos$WorkerId)#
#
#And merge back in#
mydata <- merge(mydata, participantNos, by=c('WorkerId','trueSubNo'), all.x=TRUE)#
#((if you were willow-only, you pass this part!))#
mydata[is.na(mydata$HasTurk),]$isDup <- FALSE#
##################################################################
## Recode and clean data & conditions
mydata
names(mydata)
mydata$surpriseResponse
mydata$LeftPic
length(unique(mydata$Paycode)) #145#
#
mydata <- mydata[mydata$Answer.country == "USA" &#
		mydata$Answer.English == "yes" &#
		#mydata$wasError < 1 & #You did the task!#
		mydata$isDup == FALSE,] #You didn't take it before!!#
#Throw out people who only had turk data (no code given...)#
mydata <- mydata[!is.na(mydata$HasWillow),]#
#
length(unique(mydata$Paycode)) #132
mydata$choseGoal <- FALSE#
mydata[mydata$LeftPic == "Goal" & mydata$surpriseResponse == "left",]$choseGoal <- TRUE
mydata$choseGoal
mydata[mydata$LeftPic == "Path" & mydata$surpriseResponse == "right",]$choseGoal <- TRUE
mydata$choseGoal
alldata <- mydata#
#Report S counts#
length(unique(alldata$Paycode))#
with(firstdata, tapply(Paycode, list(Condition), length))
length(unique(alldata$Paycode))#
with(alldata, tapply(Paycode, list(Condition), length))
foo <- with(alldata, tapply(choseGoal, list(Condition), mean, na.rm=TRUE), drop=TRUE)
foo
goo <- with(alldata, tapply(choseGoal, list(Condition), stderr), drop=TRUE)
goo
Preliminary data read-in & cleaning/reshaping.  Note below where interesting stuff starts.  Look for#
# 'STEVE START HERE'#
#
setwd("/Users/mekline/Dropbox/_Projects/NRSA/Adult Studies/ArcReach/Analysis Predict")#
library(languageR)#
library(stringr)#
library(lme4)#
library(multcomp)#
library(binom)#
mean.na.rm <- function(x) { mean(x,na.rm=T) }#
stderr <- function(x) sqrt(var(x)/length(x))#
#
stderr_binom <-function(x){#
	p = sum(x)/length(x)#
	q = 1-p#
	sqrt(p*q/length(x))#
}#
#
confint <- function(x){#
	qt(0.975,df=length(x)-1)*stderr(x)#
}#
#
confint_norm_lower <- function(x){#
	foo <- binom.confint(sum(x), length(x), conf.level = 0.95, methods = "asymptotic")#
	foo[5]#
#
}#
#
confint_norm_upper <- function(x){#
	foo <- binom.confint(sum(x), length(x), conf.level = 0.95, methods = "asymptotic")#
	foo[6]#
#
}#
#
turk.files <- list.files('batch')#
willow.files <- list.files('log')#
################################################################
## Read in the data files from turk#
list.number <- 1;  # keep track of the list number#
turkdata <- data.frame(NULL)#
for(f in turk.files) {#
	cat(f)#
	tmp <- read.csv(paste('batch/',f, sep=''), header=T)#
	tmp$turkfile <- f#
	turkdata <- rbind(turkdata, tmp)#
#
	list.number <- list.number + 1#
}#
#
dropped.columns <- c("HITId", "HITTypeId","Title", "Description", "Keywords", "Reward", "CreationTime", "MaxAssignments", 	"RequesterAnnotation", "AssignmentDurationInSeconds", "AutoApprovalDelayInSeconds", "Expiration", "NumberOfSimilarHITs", 	"LifetimeInSeconds", "AssignmentId", "AcceptTime", "AutoApprovalTime", "ApprovalTime", "RejectionTime", 	"RequesterFeedback", "AssignmentStatus","LifetimeApprovalRate", "Last30DaysApprovalRate","Last7DaysApprovalRate" )#
#
turkdata <- turkdata[, setdiff(names(turkdata), dropped.columns)]#
#
################################################################
## Read in the data files from willow#
willowdata <- data.frame(NULL)#
for (w in willow.files) {#
	cat(w)#
	tmp <- read.csv(paste('log/',w, sep=''), header=T)#
	tmp$willowfile <- w#
	tmp$willowcode <- paste(unlist(str_extract_all(w,"[0-9+]")),collapse='')#
	willowdata <- rbind(willowdata, tmp)#
}#
#
#Remove extra header lines...#
willowdata <- willowdata[willowdata$Paycode != "Paycode",]#
################################################################
## Merge the information from turk and willow!  #
#
#How many do we start with?#
length(unique(willowdata$Paycode)) #107#
length(unique(turkdata$WorkerId)) #53#
#Save and standardize#
turkdata$Paycode <- turkdata$Answer.payCode#
turkdata$Answer.payCode <- NA#
turkdata <- turkdata[,setdiff(names(turkdata), c("Answer.payCode"))]#
turkdata$HasTurk <- TRUE#
willowdata$HasWillow <- TRUE#
#
mydata <- NULL#
mydata <- merge(willowdata, turkdata, by=c("Paycode"), all.x=TRUE, all.y=TRUE)#
#
#If your line doesn't have Willow, you are a useless unmatched Turk entry!#
#
#Check who these jokers are!  Will want to make double sure they have not taken the survey twice...#
mydata[is.na(mydata$HasWillow),]$Paycode#
mydata[is.na(mydata$HasWillow),]$WorkerId#
##################################################################
## Check through the data for compliance and multiple takers (the general full-set solution!! #
## Use this for actually running the analysis!#
#
#If your line doesn't have Turk, we'll give you a free pass on language, country, and video presentation#
mydata[is.na(mydata$HasTurk),]$Answer.English <- "yes"#
mydata[is.na(mydata$HasTurk),]$Answer.country <- "USA"#
mydata[is.na(mydata$HasTurk),]$VideoProblem <- FALSE#
#
#Come up with a true subject labeling order#
mydata$willowSubNo <- as.numeric(as.character(mydata$willowSubNo))#
trueSub <- unique(mydata[,c('willowcode','willowSubNo', 'Paycode')])#
trueSub <- trueSub[order(trueSub$willowcode, trueSub$willowSubNo),]#
trueSub$trueSubNo <- 1:nrow(trueSub)#
#
#And add it back to the dataframe#
mydata <- merge(mydata, trueSub, by=c('willowcode','willowSubNo', 'Paycode'))#
#
#Now check if any Turk workerIDs have MORE THAN ONE subno#
participantNos <- aggregate(mydata$trueSubNo, by=list(mydata$WorkerId, mydata$trueSubNo), unique)#
participantNos <- participantNos[,1:2]#
names(participantNos) <- c('WorkerId','trueSubNo')#
#
#Make sure it's in the correct order!!#
participantNos <- participantNos[order(participantNos$trueSubNo),]#
participantNos$isDup <- duplicated(participantNos$WorkerId)#
#
#And merge back in#
mydata <- merge(mydata, participantNos, by=c('WorkerId','trueSubNo'), all.x=TRUE)#
#((if you were willow-only, you pass this part!))#
mydata[is.na(mydata$HasTurk),]$isDup <- FALSE#
##################################################################
## Recode and clean data & conditions#
#
#Check for number of legal responses given in a single session!!#
#
#Drop for analysis#
#(Remember, Willow-onliers got a free pass on the first 3 here...)#
#
length(unique(mydata$Paycode)) #81#
#
mydata <- mydata[mydata$Answer.country == "USA" &#
		mydata$Answer.English == "yes" &#
		#mydata$wasError < 1 & #You did the task!#
		mydata$isDup == FALSE,] #You didn't take it before!!#
#Throw out people who only had turk data (no code given...)#
mydata <- mydata[!is.na(mydata$HasWillow),]#
#
length(unique(mydata$Paycode)) #78#
##################################################################
## CREATE SCORE AND TABULATE DESCRIPTIVES#
#
#Which thing did they pick?#
#
mydata$choseGoal <- FALSE#
mydata[mydata$LeftPic == "Goal" & mydata$surpriseResponse == "left",]$choseGoal <- TRUE#
mydata[mydata$LeftPic == "Path" & mydata$surpriseResponse == "right",]$choseGoal <- TRUE#
#
alldata <- mydata
Preliminary data read-in & cleaning/reshaping.  Note below where interesting stuff starts.  Look for#
# 'STEVE START HERE'#
#
setwd("/Users/mekline/Dropbox/_Projects/NRSA/Adult Studies/ArcReach/Analysis Predict")#
library(languageR)#
library(stringr)#
library(lme4)#
library(multcomp)#
library(binom)#
mean.na.rm <- function(x) { mean(x,na.rm=T) }#
stderr <- function(x) sqrt(var(x)/length(x))#
#
stderr_binom <-function(x){#
	p = sum(x)/length(x)#
	q = 1-p#
	sqrt(p*q/length(x))#
}#
#
confint <- function(x){#
	qt(0.975,df=length(x)-1)*stderr(x)#
}#
#
confint_norm_lower <- function(x){#
	foo <- binom.confint(sum(x), length(x), conf.level = 0.95, methods = "asymptotic")#
	foo[5]#
#
}#
#
confint_norm_upper <- function(x){#
	foo <- binom.confint(sum(x), length(x), conf.level = 0.95, methods = "asymptotic")#
	foo[6]#
#
}#
#
turk.files <- list.files('batch')#
willow.files <- list.files('log')#
################################################################
## Read in the data files from turk#
list.number <- 1;  # keep track of the list number#
turkdata <- data.frame(NULL)#
for(f in turk.files) {#
	cat(f)#
	tmp <- read.csv(paste('batch/',f, sep=''), header=T)#
	tmp$turkfile <- f#
	turkdata <- rbind(turkdata, tmp)#
#
	list.number <- list.number + 1#
}#
#
dropped.columns <- c("HITId", "HITTypeId","Title", "Description", "Keywords", "Reward", "CreationTime", "MaxAssignments", 	"RequesterAnnotation", "AssignmentDurationInSeconds", "AutoApprovalDelayInSeconds", "Expiration", "NumberOfSimilarHITs", 	"LifetimeInSeconds", "AssignmentId", "AcceptTime", "AutoApprovalTime", "ApprovalTime", "RejectionTime", 	"RequesterFeedback", "AssignmentStatus","LifetimeApprovalRate", "Last30DaysApprovalRate","Last7DaysApprovalRate" )#
#
turkdata <- turkdata[, setdiff(names(turkdata), dropped.columns)]#
#
################################################################
## Read in the data files from willow#
willowdata <- data.frame(NULL)#
for (w in willow.files) {#
	cat(w)#
	tmp <- read.csv(paste('log/',w, sep=''), header=T)#
	tmp$willowfile <- w#
	tmp$willowcode <- paste(unlist(str_extract_all(w,"[0-9+]")),collapse='')#
	willowdata <- rbind(willowdata, tmp)#
}#
#
#Remove extra header lines...#
willowdata <- willowdata[willowdata$Paycode != "Paycode",]#
################################################################
## Merge the information from turk and willow!  #
#
#How many do we start with?#
length(unique(willowdata$Paycode)) #107#
length(unique(turkdata$WorkerId)) #53#
#Save and standardize#
turkdata$Paycode <- turkdata$Answer.payCode#
turkdata$Answer.payCode <- NA#
turkdata <- turkdata[,setdiff(names(turkdata), c("Answer.payCode"))]#
turkdata$HasTurk <- TRUE#
willowdata$HasWillow <- TRUE#
#
mydata <- NULL#
mydata <- merge(willowdata, turkdata, by=c("Paycode"), all.x=TRUE, all.y=TRUE)#
#
#If your line doesn't have Willow, you are a useless unmatched Turk entry!#
#
#Check who these jokers are!  Will want to make double sure they have not taken the survey twice...#
mydata[is.na(mydata$HasWillow),]$Paycode#
mydata[is.na(mydata$HasWillow),]$WorkerId#
##################################################################
## Check through the data for compliance and multiple takers (the general full-set solution!! #
## Use this for actually running the analysis!#
#
#If your line doesn't have Turk, we'll give you a free pass on language, country, and video presentation#
mydata[is.na(mydata$HasTurk),]$Answer.English <- "yes"#
mydata[is.na(mydata$HasTurk),]$Answer.country <- "USA"#
mydata[is.na(mydata$HasTurk),]$VideoProblem <- FALSE#
#
#Come up with a true subject labeling order#
mydata$willowSubNo <- as.numeric(as.character(mydata$willowSubNo))#
trueSub <- unique(mydata[,c('willowcode','willowSubNo', 'Paycode')])#
trueSub <- trueSub[order(trueSub$willowcode, trueSub$willowSubNo),]#
trueSub$trueSubNo <- 1:nrow(trueSub)#
#
#And add it back to the dataframe#
mydata <- merge(mydata, trueSub, by=c('willowcode','willowSubNo', 'Paycode'))#
#
#Now check if any Turk workerIDs have MORE THAN ONE subno#
participantNos <- aggregate(mydata$trueSubNo, by=list(mydata$WorkerId, mydata$trueSubNo), unique)#
participantNos <- participantNos[,1:2]#
names(participantNos) <- c('WorkerId','trueSubNo')#
#
#Make sure it's in the correct order!!#
participantNos <- participantNos[order(participantNos$trueSubNo),]#
participantNos$isDup <- duplicated(participantNos$WorkerId)#
#
#And merge back in#
mydata <- merge(mydata, participantNos, by=c('WorkerId','trueSubNo'), all.x=TRUE)#
#((if you were willow-only, you pass this part!))#
mydata[is.na(mydata$HasTurk),]$isDup <- FALSE#
##################################################################
## Recode and clean data & conditions#
#
#Check for number of legal responses given in a single session!!#
#
#Drop for analysis#
#(Remember, Willow-onliers got a free pass on the first 3 here...)#
#
length(unique(mydata$Paycode)) #81#
#
mydata <- mydata[mydata$Answer.country == "USA" &#
		mydata$Answer.English == "yes" &#
		#mydata$wasError < 1 & #You did the task!#
		mydata$isDup == FALSE,] #You didn't take it before!!#
#Throw out people who only had turk data (no code given...)#
mydata <- mydata[!is.na(mydata$HasWillow),]#
#
length(unique(mydata$Paycode)) #78#
##################################################################
## CREATE SCORE AND TABULATE DESCRIPTIVES#
#
#Which thing did they pick?#
#
mydata$choseGoal <- FALSE#
mydata[mydata$LeftPic == "Goal" & mydata$surpriseResponse == "left",]$choseGoal <- TRUE#
mydata[mydata$LeftPic == "Path" & mydata$surpriseResponse == "right",]$choseGoal <- TRUE#
#
alldata <- mydata
Report S counts#
length(unique(alldata$Paycode))#
with(alldata, tapply(Paycode, list(Condition), length))#
#Report mean and standard error of SCORES#
foo <- with(alldata, tapply(choseGoal, list(Condition), mean, na.rm=TRUE), drop=TRUE)#
goo <- with(alldata, tapply(choseGoal, list(Condition), stderr), drop=TRUE)
foo
xtabs(formula = choseGoal ~ Condition, data = alldata)
14/27
mygrid <- c(14,9,10,13, 13, 14, 18, 14)
dim(mygrid) <- c(2, 4)
mygrid
dim(mygrid) <- c(4, 2)
mygrid
chisq.test(mygrid)
mylittle <- c(10,18,13,14)
dim(mylittle) <- c(2,2)
mylittle
chisq.test(mylittle)
fisher.test(mylittle)
Preliminary data read-in & cleaning/reshaping.  Note below where interesting stuff starts.  Look for#
# 'STEVE START HERE'#
#
setwd("/Users/mekline/Dropbox/_Projects/Gesture Comprehension/Gesture-to-Comprehension/Analysis 1/")#
library(languageR)#
library(stringr)#
library(lme4)#
library(multcomp)#
library(binom)#
mean.na.rm <- function(x) { mean(x,na.rm=T) }#
stderr <- function(x) sqrt(var(x)/length(x))#
#
stderr_binom <-function(x){#
	p = sum(x)/length(x)#
	q = 1-p#
	sqrt(p*q/length(x))#
}#
#
confint <- function(x){#
	qt(0.975,df=length(x)-1)*stderr(x)#
}#
#
confint_norm_lower <- function(x){#
	foo <- binom.confint(sum(x), length(x), conf.level = 0.95, methods = "asymptotic")#
	foo[5]#
#
}#
#
confint_norm_upper <- function(x){#
	foo <- binom.confint(sum(x), length(x), conf.level = 0.95, methods = "asymptotic")#
	foo[6]#
#
}#
#
turk.files <- list.files('batch')#
willow.files <- list.files('log')#
################################################################
## Read in the data files from turk#
list.number <- 1;  # keep track of the list number#
turkdata <- data.frame(NULL)#
for(f in turk.files) {#
	cat(f)#
	tmp <- read.csv(paste('batch/',f, sep=''), header=T)#
	tmp$turkfile <- f#
	turkdata <- rbind(turkdata, tmp)#
#
	list.number <- list.number + 1#
}#
#
dropped.columns <- c("HITId", "HITTypeId","Title", "Description", "Keywords", "Reward", "CreationTime", "MaxAssignments", 	"RequesterAnnotation", "AssignmentDurationInSeconds", "AutoApprovalDelayInSeconds", "Expiration", "NumberOfSimilarHITs", 	"LifetimeInSeconds", "AssignmentId", "AcceptTime", "AutoApprovalTime", "ApprovalTime", "RejectionTime", 	"RequesterFeedback", "AssignmentStatus","LifetimeApprovalRate", "Last30DaysApprovalRate","Last7DaysApprovalRate" )#
#
turkdata <- turkdata[, setdiff(names(turkdata), dropped.columns)]#
#
################################################################
## Read in the data files from willow#
willowdata <- data.frame(NULL)#
for (w in willow.files) {#
	cat(w)#
	tmp <- read.csv(paste('log/',w, sep=''), header=T)#
	tmp$willowfile <- w#
	tmp$willowcode <- paste(unlist(str_extract_all(w,"[0-9+]")),collapse='')#
	willowdata <- rbind(willowdata, tmp)#
}#
#
#Remove extra header lines...#
willowdata <- willowdata[willowdata$Paycode != "Paycode",]#
################################################################
## Merge the information from turk and willow!  #
#
#How many do we start with?#
length(unique(willowdata$Paycode)) #55#
length(unique(turkdata$WorkerId)) #30#
#Save and standardize#
turkdata$Paycode <- turkdata$Answer.payCode#
turkdata$Answer.payCode <- NA#
turkdata <- turkdata[,setdiff(names(turkdata), c("Answer.payCode"))]#
turkdata$HasTurk <- TRUE#
willowdata$HasWillow <- TRUE#
#
mydata <- NULL#
mydata <- merge(willowdata, turkdata, by=c("Paycode"), all.x=TRUE, all.y=TRUE)#
#
#If your line doesn't have Willow, you are a useless unmatched Turk entry!#
#
#Check who these jokers are!  Will want to make double sure they have not taken the survey twice...#
mydata[is.na(mydata$HasWillow),]$Paycode#
mydata[is.na(mydata$HasWillow),]$WorkerId#
##################################################################
## Check through the data for compliance and multiple takers (the general full-set solution!! #
## Use this for actually running the analysis!#
#
#If your line doesn't have Turk, we'll give you a free pass on language, country, and video presentation#
mydata[is.na(mydata$HasTurk),]$Answer.English <- "yes"#
mydata[is.na(mydata$HasTurk),]$Answer.country <- "USA"#
mydata[is.na(mydata$HasTurk),]$VideoProblem <- FALSE#
#
#Come up with a true subject labeling order#
mydata$willowSubNo <- as.numeric(as.character(mydata$willowSubNo))#
trueSub <- unique(mydata[,c('willowcode','willowSubNo', 'Paycode')])#
trueSub <- trueSub[order(trueSub$willowcode, trueSub$willowSubNo),]#
trueSub$trueSubNo <- 1:nrow(trueSub)#
#
#And add it back to the dataframe#
mydata <- merge(mydata, trueSub, by=c('willowcode','willowSubNo', 'Paycode'))#
#
#Now check if any Turk workerIDs have MORE THAN ONE subno#
participantNos <- aggregate(mydata$trueSubNo, by=list(mydata$WorkerId, mydata$trueSubNo), unique)#
participantNos <- participantNos[,1:2]#
names(participantNos) <- c('WorkerId','trueSubNo')#
#
#Make sure it's in the correct order!!#
participantNos <- participantNos[order(participantNos$trueSubNo),]#
participantNos$isDup <- duplicated(participantNos$WorkerId)#
#
#And merge back in#
mydata <- merge(mydata, participantNos, by=c('WorkerId','trueSubNo'), all.x=TRUE)#
#((if you were willow-only, you pass this part!))#
mydata[is.na(mydata$HasTurk),]$isDup <- FALSE#
##################################################################
## Recode and clean data & conditions#
#
#Check for number of legal responses given in a single session!!#
mydata$GotAnswer <- 0#
mydata[is.na(mydata$wasError),]$wasError <- '1' #Fix up missing values#
#
mydata[mydata$Response == 'Yes' & mydata$wasError == '0',]$GotAnswer <- 1#
mydata[mydata$Response == 'No' & mydata$wasError == '0',]$GotAnswer <- 1#
participant.responsecount <- aggregate(mydata$GotAnswer, by=list(mydata$Paycode), sum)#
names(participant.responsecount) <- c("Paycode", "LegalAnswers")#
mydata <- merge(mydata, participant.responsecount, by=c("Paycode"), all.x=TRUE)#
#Drop for analysis#
#(Remember, Willow-onliers got a free pass on the first 3 here...)#
#
length(unique(mydata$Paycode)) #112
mydata <- mydata[mydata$Answer.country == "USA" &#
		mydata$Answer.English == "yes" &#
		mydata$LegalAnswers > 65 & #You did at least 3/4 of the trials#
		mydata$VideoProblem == FALSE & #No reported lag/issue#
		mydata$isDup == FALSE,] #You didn't take it before!!#
#Throw out people who only had turk data (no code given...)#
mydata <- mydata[!is.na(mydata$HasWillow),]#
#
length(unique(mydata$Paycode)) #53
mydata$wasCorrect <- as.numeric(as.character(mydata$wasCorrect))#
alldata <- mydata #
#
#What about people who didn't just ceiling out of the task?#
personScores <- aggregate(alldata$wasCorrect, by=list(alldata$Paycode), mean.na.rm)#
names(personScores) <-c("Paycode","overallScore")#
alldata <- merge(alldata, personScores)#
#
lowdata <- alldata[alldata$overallScore < 0.75,]#
hidata <- alldata[alldata$overallScore >= 0.9,]#
#
#What about people who paid more/less attention to the flanking memory task?#
responseAScores <- aggregate(as.numeric(as.character(alldata$wordResponseA)), by=list(alldata$Paycode), mean.na.rm)#
names(responseAScores) <-c("Paycode","AScore")#
responseBScores <- aggregate(as.numeric(as.character(alldata$wordResponseB)), by=list(alldata$Paycode), mean.na.rm)#
names(responseBScores) <-c("Paycode","BScore")#
wordResponses <- merge(responseAScores, responseBScores, by=c("Paycode"))#
wordResponses$wordResponse <- wordResponses$AScore + wordResponses$BScore#
alldata <- merge(alldata, wordResponses)#
#
worddata <- alldata[alldata$wordResponse > 8,]#
middata <- alldata[alldata$wordResponse < 8,]#
baddata <- alldata[alldata$wordResponse < 6,]#
#
#And look at the 1st and 2nd halves of the data!#
alldata$trialNo <- as.numeric(as.character(alldata$trialNo))#
firstdata <- alldata[alldata$trialNo < 37 ,]#
lastdata <- alldata[alldata$trialNo > 20,]
Calculate SCORE per person per condition#
Scores <- aggregate(alldata$wasCorrect, by=list(alldata$Paycode, alldata$sentType, alldata$sentOrder), sum)#
names(Scores) <- c("Paycode", "sentType",  "sentOrder", "CorrectScore")#
alldata <- merge(alldata, Scores, by=c("Paycode", "sentType", "sentOrder"))#
#
#TrialScores#
TrialScores <- aggregate(alldata$wasCorrect, by=list(alldata$trialNo, alldata$sentType, alldata$sentOrder), mean.na.rm)#
names(TrialScores) <- c("Paycode", "sentType",  "sentOrder", "CorrectScore")#
#Calculate SCORE per person per condition - LOWDATA#
lowScores <- aggregate(lowdata$wasCorrect, by=list(lowdata$Paycode, lowdata$sentType, lowdata$sentOrder), sum)#
names(lowScores) <- c("Paycode", "sentType",  "sentOrder", "CorrectScore")#
lowdata <- merge(lowdata, lowScores, by=c("Paycode", "sentType",  "sentOrder"))#
#
#Calculate SCORE per person per condition - HIDATA#
hiScores <- aggregate(hidata$wasCorrect, by=list(hidata$Paycode, hidata$sentType, hidata$sentOrder), sum)#
names(hiScores) <- c("Paycode", "sentType",  "sentOrder", "CorrectScore")#
hidata <- merge(hidata, hiScores, by=c("Paycode", "sentType",  "sentOrder"))#
#
#Calculate SCORE per person per condition - WORDDATA#
wordScores <- aggregate(worddata$wasCorrect, by=list(worddata$Paycode, worddata$sentType, worddata$sentOrder), sum)#
names(wordScores) <- c("Paycode", "sentType",  "sentOrder", "CorrectScore")#
worddata <- merge(worddata, wordScores, by=c("Paycode", "sentType",  "sentOrder"))#
#
midScores <- aggregate(middata$wasCorrect, by=list(middata$Paycode, middata$sentType, middata$sentOrder), sum)#
names(midScores) <- c("Paycode", "sentType",  "sentOrder", "CorrectScore")#
middata <- merge(middata, midScores, by=c("Paycode", "sentType",  "sentOrder"))#
#
badScores <- aggregate(baddata$wasCorrect, by=list(baddata$Paycode, baddata$sentType, baddata$sentOrder), sum)#
names(badScores) <- c("Paycode", "sentType",  "sentOrder", "CorrectScore")#
baddata <- merge(baddata, badScores, by=c("Paycode", "sentType",  "sentOrder"))#
#
#Calculate SCORE per person per condition - first/last data#
firstScores <- aggregate(firstdata$wasCorrect, by=list(firstdata$Paycode, firstdata$sentType, firstdata$sentOrder), sum)#
names(firstScores) <- c("Paycode", "sentType",  "sentOrder", "CorrectScore")#
firstdata <- merge(firstdata, firstScores, by=c("Paycode", "sentType",  "sentOrder"))#
#
lastScores <- aggregate(lastdata$wasCorrect, by=list(lastdata$Paycode, lastdata$sentType, lastdata$sentOrder), sum)#
names(lastScores) <- c("Paycode", "sentType",  "sentOrder", "CorrectScore")#
lastdata <- merge(lastdata, lastScores, by=c("Paycode", "sentType",  "sentOrder"))
Report S counts#
length(unique(alldata$Paycode))
Report mean and standard error of SCORES#
with(Scores, tapply(CorrectScore, list(sentType, sentOrder), mean, na.rm=TRUE), drop=TRUE)#
with(Scores, tapply(CorrectScore, list(sentType, sentOrder), stderr), drop=TRUE)
with(lastScores, tapply(CorrectScore, list(sentType, sentOrder), mean, na.rm=TRUE), drop=TRUE)#
with(lastScores, tapply(CorrectScore, list(sentType, sentOrder), stderr), drop=TRUE)
lastdata <- alldata[alldata$trialNo > 10,]
lastScores <- aggregate(lastdata$wasCorrect, by=list(lastdata$Paycode, lastdata$sentType, lastdata$sentOrder), sum)#
names(lastScores) <- c("Paycode", "sentType",  "sentOrder", "CorrectScore")#
lastdata <- merge(lastdata, lastScores, by=c("Paycode", "sentType",  "sentOrder"))
with(lastScores, tapply(CorrectScore, list(sentType, sentOrder), mean, na.rm=TRUE), drop=TRUE)#
with(lastScores, tapply(CorrectScore, list(sentType, sentOrder), stderr), drop=TRUE)
72/4
lastdata <- alldata[alldata$trialNo > 18,]
lastScores <- aggregate(lastdata$wasCorrect, by=list(lastdata$Paycode, lastdata$sentType, lastdata$sentOrder), sum)#
names(lastScores) <- c("Paycode", "sentType",  "sentOrder", "CorrectScore")#
lastdata <- merge(lastdata, lastScores, by=c("Paycode", "sentType",  "sentOrder"))
with(lastScores, tapply(CorrectScore, list(sentType, sentOrder), mean, na.rm=TRUE), drop=TRUE)#
with(lastScores, tapply(CorrectScore, list(sentType, sentOrder), stderr), drop=TRUE)
lasttransdata <- lastdata[lastdata$sentType == "AO"| lastdata$sentType == "IO",]
lastgesturecomp_maximal_model <- lmer(wasCorrect ~ sentType*sentOrder + (sentType*sentOrder|stimNo) + (sentType*sentOrder|Paycode), data=lasttransdata, family="binomial")#
summary(lastgesturecomp_maximal_model)
lastgesturecomp_nointer_model <- lmer(wasCorrect ~ sentType+sentOrder + (sentType*sentOrder|stimNo) + (sentType*sentOrder|Paycode), data=lasttransdata, family="binomial")#
anova(lastgesturecomp_maximal_model, lastgesturecomp_nointer_model)
with(lastScores, tapply(CorrectScore, list(sentType, sentOrder), confint_norm_lower), drop=TRUE)#
with(lastScores, tapply(CorrectScore, list(sentType, sentOrder), confint_norm_upper), drop=TRUE)
foo = c(1,2,3,4,5,6,7)
confint(sum(foo), length(foo), conf.level = 0.95, methods = "asymptotic")
help confint
help(confint)
confint(lastgesturecomp_maximal_model)
lastScores
Let's try an anova??#
fit <- lm(CorrectScore ~ sentType * sentOrder + (1/Paycode), lastScores)#
anova(fit)
fit <- lm(CorrectScore ~ sentType * sentOrder + (1/stimNo), lastScores)#
anova(fit)
fit <- lm(CorrectScore ~ sentType * sentOrder + (1/Paycode), lastScores)#
anova(fit)
lastItemScores <- aggregate(lastdata$wasCorrect, by=list(lastdata$stimNo, lastdata$sentType, lastdata$sentOrder), sum)#
names(lastItemScores) <- c("stimNo", "sentType",  "sentOrder", "CorrectScore")
lastItemScores
lastTransScores <- lastScores[lastScores$sentType == "AO" | lastScores$sentType == "IO"]
names(lastScores)
lastTransScores <- lastScores[lastScores$sentType == "AO" | lastScores$sentType == "IO",]
lastTransScores
lastScores
fit <- lm(CorrectScore ~ sentType * sentOrder + (1/Paycode), lastTransScores)
anova(fit)
lastItemScores <- aggregate(lastdata$wasCorrect, by=list(lastdata$stimNo, lastdata$sentType, lastdata$sentOrder), sum)#
names(lastItemScores) <- c("stimNo", "sentType",  "sentOrder", "CorrectScore")
lastItemTransScores <- lastItemScores[lastItemScores$sentType == "AO" | lastItemScores$sentType == "IO",]
fit <- lm(CorrectScore ~ sentType * sentOrder + (1/stimNo), lastItemTransScores)#
anova(fit)
Preliminary data read-in & cleaning/reshaping.  Note below where interesting stuff starts.  Look for#
# 'STEVE START HERE'#
#
setwd("/Users/mekline/Dropbox/_Projects/Gesture Comprehension/Gesture-to-Comprehension/Analysis 1/")#
library(languageR)#
library(stringr)#
library(lme4)#
library(multcomp)#
library(binom)#
mean.na.rm <- function(x) { mean(x,na.rm=T) }#
stderr <- function(x) sqrt(var(x)/length(x))#
#
stderr_binom <-function(x){#
	p = sum(x)/length(x)#
	q = 1-p#
	sqrt(p*q/length(x))#
}#
#
confint <- function(x){#
	qt(0.975,df=length(x)-1)*stderr(x)#
}#
#
confint_norm_lower <- function(x){#
	foo <- binom.confint(sum(x), length(x), conf.level = 0.95, methods = "asymptotic")#
	foo[5]#
#
}#
#
confint_norm_upper <- function(x){#
	foo <- binom.confint(sum(x), length(x), conf.level = 0.95, methods = "asymptotic")#
	foo[6]#
#
}#
#
turk.files <- list.files('batch')#
willow.files <- list.files('log')#
################################################################
## Read in the data files from turk#
list.number <- 1;  # keep track of the list number#
turkdata <- data.frame(NULL)#
for(f in turk.files) {#
	cat(f)#
	tmp <- read.csv(paste('batch/',f, sep=''), header=T)#
	tmp$turkfile <- f#
	turkdata <- rbind(turkdata, tmp)#
#
	list.number <- list.number + 1#
}#
#
dropped.columns <- c("HITId", "HITTypeId","Title", "Description", "Keywords", "Reward", "CreationTime", "MaxAssignments", 	"RequesterAnnotation", "AssignmentDurationInSeconds", "AutoApprovalDelayInSeconds", "Expiration", "NumberOfSimilarHITs", 	"LifetimeInSeconds", "AssignmentId", "AcceptTime", "AutoApprovalTime", "ApprovalTime", "RejectionTime", 	"RequesterFeedback", "AssignmentStatus","LifetimeApprovalRate", "Last30DaysApprovalRate","Last7DaysApprovalRate" )#
#
turkdata <- turkdata[, setdiff(names(turkdata), dropped.columns)]#
#
################################################################
## Read in the data files from willow#
willowdata <- data.frame(NULL)#
for (w in willow.files) {#
	cat(w)#
	tmp <- read.csv(paste('log/',w, sep=''), header=T)#
	tmp$willowfile <- w#
	tmp$willowcode <- paste(unlist(str_extract_all(w,"[0-9+]")),collapse='')#
	willowdata <- rbind(willowdata, tmp)#
}#
#
#Remove extra header lines...#
willowdata <- willowdata[willowdata$Paycode != "Paycode",]#
################################################################
## Merge the information from turk and willow!  #
#
#How many do we start with?#
length(unique(willowdata$Paycode)) #55#
length(unique(turkdata$WorkerId)) #30#
#Save and standardize#
turkdata$Paycode <- turkdata$Answer.payCode#
turkdata$Answer.payCode <- NA#
turkdata <- turkdata[,setdiff(names(turkdata), c("Answer.payCode"))]#
turkdata$HasTurk <- TRUE#
willowdata$HasWillow <- TRUE#
#
mydata <- NULL#
mydata <- merge(willowdata, turkdata, by=c("Paycode"), all.x=TRUE, all.y=TRUE)#
#
#If your line doesn't have Willow, you are a useless unmatched Turk entry!#
#
#Check who these jokers are!  Will want to make double sure they have not taken the survey twice...#
mydata[is.na(mydata$HasWillow),]$Paycode#
mydata[is.na(mydata$HasWillow),]$WorkerId#
##################################################################
## Check through the data for compliance and multiple takers (the general full-set solution!! #
## Use this for actually running the analysis!#
#
#If your line doesn't have Turk, we'll give you a free pass on language, country, and video presentation#
mydata[is.na(mydata$HasTurk),]$Answer.English <- "yes"#
mydata[is.na(mydata$HasTurk),]$Answer.country <- "USA"#
mydata[is.na(mydata$HasTurk),]$VideoProblem <- FALSE#
#
#Come up with a true subject labeling order#
mydata$willowSubNo <- as.numeric(as.character(mydata$willowSubNo))#
trueSub <- unique(mydata[,c('willowcode','willowSubNo', 'Paycode')])#
trueSub <- trueSub[order(trueSub$willowcode, trueSub$willowSubNo),]#
trueSub$trueSubNo <- 1:nrow(trueSub)#
#
#And add it back to the dataframe#
mydata <- merge(mydata, trueSub, by=c('willowcode','willowSubNo', 'Paycode'))#
#
#Now check if any Turk workerIDs have MORE THAN ONE subno#
participantNos <- aggregate(mydata$trueSubNo, by=list(mydata$WorkerId, mydata$trueSubNo), unique)#
participantNos <- participantNos[,1:2]#
names(participantNos) <- c('WorkerId','trueSubNo')#
#
#Make sure it's in the correct order!!#
participantNos <- participantNos[order(participantNos$trueSubNo),]#
participantNos$isDup <- duplicated(participantNos$WorkerId)#
#
#And merge back in#
mydata <- merge(mydata, participantNos, by=c('WorkerId','trueSubNo'), all.x=TRUE)#
#((if you were willow-only, you pass this part!))#
mydata[is.na(mydata$HasTurk),]$isDup <- FALSE#
##################################################################
## Recode and clean data & conditions#
#
#Check for number of legal responses given in a single session!!#
mydata$GotAnswer <- 0#
mydata[is.na(mydata$wasError),]$wasError <- '1' #Fix up missing values#
#
mydata[mydata$Response == 'Yes' & mydata$wasError == '0',]$GotAnswer <- 1#
mydata[mydata$Response == 'No' & mydata$wasError == '0',]$GotAnswer <- 1#
participant.responsecount <- aggregate(mydata$GotAnswer, by=list(mydata$Paycode), sum)#
names(participant.responsecount) <- c("Paycode", "LegalAnswers")#
mydata <- merge(mydata, participant.responsecount, by=c("Paycode"), all.x=TRUE)#
#Drop for analysis#
#(Remember, Willow-onliers got a free pass on the first 3 here...)#
#
length(unique(mydata$Paycode)) #214#
mydata <- mydata[mydata$Answer.country == "USA" &#
		mydata$Answer.English == "yes" &#
		mydata$LegalAnswers > 65 & #You did at least 3/4 of the trials#
		mydata$VideoProblem == FALSE & #No reported lag/issue#
		mydata$isDup == FALSE,] #You didn't take it before!!#
#Throw out people who only had turk data (no code given...)#
mydata <- mydata[!is.na(mydata$HasWillow),]#
#
length(unique(mydata$Paycode)) #105
CREATING DATA SUBSETS HERE#
#
mydata$wasCorrect <- as.numeric(as.character(mydata$wasCorrect))#
alldata <- mydata #
#
#What about people who didn't just ceiling out of the task?#
personScores <- aggregate(alldata$wasCorrect, by=list(alldata$Paycode), mean.na.rm)#
names(personScores) <-c("Paycode","overallScore")#
alldata <- merge(alldata, personScores)#
#
lowdata <- alldata[alldata$overallScore < 0.75,]#
#
#What about people who paid more/less attention to the flanking memory task?#
responseAScores <- aggregate(as.numeric(as.character(alldata$wordResponseA)), by=list(alldata$Paycode), mean.na.rm)#
names(responseAScores) <-c("Paycode","AScore")#
responseBScores <- aggregate(as.numeric(as.character(alldata$wordResponseB)), by=list(alldata$Paycode), mean.na.rm)#
names(responseBScores) <-c("Paycode","BScore")#
wordResponses <- merge(responseAScores, responseBScores, by=c("Paycode"))#
wordResponses$wordResponse <- wordResponses$AScore + wordResponses$BScore#
alldata <- merge(alldata, wordResponses)#
#
worddata <- alldata[alldata$wordResponse > 8,]#
middata <- alldata[alldata$wordResponse < 8,]#
#
#And look at the 1st and 2nd halves of the data!#
alldata$trialNo <- as.numeric(as.character(alldata$trialNo))#
firstdata <- alldata[alldata$trialNo < 37 ,]#
lastdata <- alldata[alldata$trialNo > 18,]#
#
##################################################################
## CREATE SCORE AND TABULATE DESCRIPTIVES#
#
#Calculate SCORE per person per condition#
Scores <- aggregate(alldata$wasCorrect, by=list(alldata$Paycode, alldata$sentType, alldata$sentOrder), sum)#
names(Scores) <- c("Paycode", "sentType",  "sentOrder", "CorrectScore")#
alldata <- merge(alldata, Scores, by=c("Paycode", "sentType", "sentOrder"))#
#
#TrialScores#
TrialScores <- aggregate(alldata$wasCorrect, by=list(alldata$trialNo, alldata$sentType, alldata$sentOrder), mean.na.rm)#
names(TrialScores) <- c("Paycode", "sentType",  "sentOrder", "CorrectScore")#
#Calculate SCORE per person per condition - LOWDATA#
lowScores <- aggregate(lowdata$wasCorrect, by=list(lowdata$Paycode, lowdata$sentType, lowdata$sentOrder), sum)#
names(lowScores) <- c("Paycode", "sentType",  "sentOrder", "CorrectScore")#
lowdata <- merge(lowdata, lowScores, by=c("Paycode", "sentType",  "sentOrder"))#
#
#Calculate SCORE per person per condition - WORDDATA#
wordScores <- aggregate(worddata$wasCorrect, by=list(worddata$Paycode, worddata$sentType, worddata$sentOrder), sum)#
names(wordScores) <- c("Paycode", "sentType",  "sentOrder", "CorrectScore")#
worddata <- merge(worddata, wordScores, by=c("Paycode", "sentType",  "sentOrder"))#
#
midScores <- aggregate(middata$wasCorrect, by=list(middata$Paycode, middata$sentType, middata$sentOrder), sum)#
names(midScores) <- c("Paycode", "sentType",  "sentOrder", "CorrectScore")#
middata <- merge(middata, midScores, by=c("Paycode", "sentType",  "sentOrder"))#
#
#Calculate SCORE per person per condition - first/last data#
firstScores <- aggregate(firstdata$wasCorrect, by=list(firstdata$Paycode, firstdata$sentType, firstdata$sentOrder), sum)#
names(firstScores) <- c("Paycode", "sentType",  "sentOrder", "CorrectScore")#
firstdata <- merge(firstdata, firstScores, by=c("Paycode", "sentType",  "sentOrder"))#
#
lastScores <- aggregate(lastdata$wasCorrect, by=list(lastdata$Paycode, lastdata$sentType, lastdata$sentOrder), sum)#
names(lastScores) <- c("Paycode", "sentType",  "sentOrder", "CorrectScore")#
lastdata <- merge(lastdata, lastScores, by=c("Paycode", "sentType",  "sentOrder"))#
#
##################################################################
## REPORT DESCRIPTIVES
with(lastScores, tapply(CorrectScore, list(sentType, sentOrder), mean, na.rm=TRUE), drop=TRUE)#
with(lastScores, tapply(CorrectScore, list(sentType, sentOrder), stderr), drop=TRUE)
lastdata <- alldata[alldata$trialNo > 37,]
lastScores <- aggregate(lastdata$wasCorrect, by=list(lastdata$Paycode, lastdata$sentType, lastdata$sentOrder), sum)#
names(lastScores) <- c("Paycode", "sentType",  "sentOrder", "CorrectScore")#
lastdata <- merge(lastdata, lastScores, by=c("Paycode", "sentType",  "sentOrder"))
with(lastScores, tapply(CorrectScore, list(sentType, sentOrder), mean, na.rm=TRUE), drop=TRUE)
lastdata <- alldata[alldata$trialNo > 10,]
lastScores <- aggregate(lastdata$wasCorrect, by=list(lastdata$Paycode, lastdata$sentType, lastdata$sentOrder), sum)#
names(lastScores) <- c("Paycode", "sentType",  "sentOrder", "CorrectScore")#
lastdata <- merge(lastdata, lastScores, by=c("Paycode", "sentType",  "sentOrder"))
with(lastScores, tapply(CorrectScore, list(sentType, sentOrder), mean, na.rm=TRUE), drop=TRUE)
Try again removing "Same" items#
diffdata <- alldata[alldata$changeType !="Same",]#
#Calculate SCORE per person per condition#
diffScores <- aggregate(diffdata$wasCorrect, by=list(diffdata$Paycode, diffdata$sentType, diffdata$sentOrder), sum)#
names(diffScores) <- c("Paycode", "sentType",  "sentOrder", "CorrectScore")#
diffdata <- merge(diffdata, Scores, by=c("Paycode", "sentType", "sentOrder"))#
#
with(diffScores, tapply(CorrectScore, list(sentType, sentOrder), mean, na.rm=TRUE), drop=TRUE)#
with(diffScores, tapply(CorrectScore, list(sentType, sentOrder), stderr), drop=TRUE)
revdata <- alldata[alldata$changeType == "Reversal",]#
#Calculate SCORE per person per condition#
revScores <- aggregate(revdata$wasCorrect, by=list(revdata$Paycode, revdata$sentType, revdata$sentOrder), sum)#
names(revScores) <- c("Paycode", "sentType",  "sentOrder", "CorrectScore")#
revdata <- merge(revdata, Scores, by=c("Paycode", "sentType", "sentOrder"))#
#
with(revScores, tapply(CorrectScore, list(sentType, sentOrder), mean, na.rm=TRUE), drop=TRUE)#
with(revScores, tapply(CorrectScore, list(sentType, sentOrder), stderr), drop=TRUE)
Preliminary data read-in & cleaning/reshaping.  Note below where interesting stuff starts.  Look for#
# 'STEVE START HERE'#
#
setwd("/Users/mekline/Dropbox/_Projects/Gesture Comprehension/Gesture-to-Comprehension/Analysis 1/")#
library(languageR)#
library(stringr)#
library(lme4)#
library(multcomp)#
library(binom)#
mean.na.rm <- function(x) { mean(x,na.rm=T) }#
stderr <- function(x) sqrt(var(x)/length(x))#
#
stderr_binom <-function(x){#
	p = sum(x)/length(x)#
	q = 1-p#
	sqrt(p*q/length(x))#
}#
#
confint <- function(x){#
	qt(0.975,df=length(x)-1)*stderr(x)#
}#
#
confint_norm_lower <- function(x){#
	foo <- binom.confint(sum(x), length(x), conf.level = 0.95, methods = "asymptotic")#
	foo[5]#
#
}#
#
confint_norm_upper <- function(x){#
	foo <- binom.confint(sum(x), length(x), conf.level = 0.95, methods = "asymptotic")#
	foo[6]#
#
}#
#
turk.files <- list.files('batch')#
willow.files <- list.files('log')#
################################################################
## Read in the data files from turk#
list.number <- 1;  # keep track of the list number#
turkdata <- data.frame(NULL)#
for(f in turk.files) {#
	cat(f)#
	tmp <- read.csv(paste('batch/',f, sep=''), header=T)#
	tmp$turkfile <- f#
	turkdata <- rbind(turkdata, tmp)#
#
	list.number <- list.number + 1#
}#
#
dropped.columns <- c("HITId", "HITTypeId","Title", "Description", "Keywords", "Reward", "CreationTime", "MaxAssignments", 	"RequesterAnnotation", "AssignmentDurationInSeconds", "AutoApprovalDelayInSeconds", "Expiration", "NumberOfSimilarHITs", 	"LifetimeInSeconds", "AssignmentId", "AcceptTime", "AutoApprovalTime", "ApprovalTime", "RejectionTime", 	"RequesterFeedback", "AssignmentStatus","LifetimeApprovalRate", "Last30DaysApprovalRate","Last7DaysApprovalRate" )#
#
turkdata <- turkdata[, setdiff(names(turkdata), dropped.columns)]#
#
################################################################
## Read in the data files from willow#
willowdata <- data.frame(NULL)#
for (w in willow.files) {#
	cat(w)#
	tmp <- read.csv(paste('log/',w, sep=''), header=T)#
	tmp$willowfile <- w#
	tmp$willowcode <- paste(unlist(str_extract_all(w,"[0-9+]")),collapse='')#
	willowdata <- rbind(willowdata, tmp)#
}#
#
#Remove extra header lines...#
willowdata <- willowdata[willowdata$Paycode != "Paycode",]#
################################################################
## Merge the information from turk and willow!  #
#
#How many do we start with?#
length(unique(willowdata$Paycode)) #55#
length(unique(turkdata$WorkerId)) #30#
#Save and standardize#
turkdata$Paycode <- turkdata$Answer.payCode#
turkdata$Answer.payCode <- NA#
turkdata <- turkdata[,setdiff(names(turkdata), c("Answer.payCode"))]#
turkdata$HasTurk <- TRUE#
willowdata$HasWillow <- TRUE#
#
mydata <- NULL#
mydata <- merge(willowdata, turkdata, by=c("Paycode"), all.x=TRUE, all.y=TRUE)#
#
#If your line doesn't have Willow, you are a useless unmatched Turk entry!#
#
#Check who these jokers are!  Will want to make double sure they have not taken the survey twice...#
mydata[is.na(mydata$HasWillow),]$Paycode#
mydata[is.na(mydata$HasWillow),]$WorkerId#
##################################################################
## Check through the data for compliance and multiple takers (the general full-set solution!! #
## Use this for actually running the analysis!#
#
#If your line doesn't have Turk, we'll give you a free pass on language, country, and video presentation#
mydata[is.na(mydata$HasTurk),]$Answer.English <- "yes"#
mydata[is.na(mydata$HasTurk),]$Answer.country <- "USA"#
mydata[is.na(mydata$HasTurk),]$VideoProblem <- FALSE#
#
#Come up with a true subject labeling order#
mydata$willowSubNo <- as.numeric(as.character(mydata$willowSubNo))#
trueSub <- unique(mydata[,c('willowcode','willowSubNo', 'Paycode')])#
trueSub <- trueSub[order(trueSub$willowcode, trueSub$willowSubNo),]#
trueSub$trueSubNo <- 1:nrow(trueSub)#
#
#And add it back to the dataframe#
mydata <- merge(mydata, trueSub, by=c('willowcode','willowSubNo', 'Paycode'))#
#
#Now check if any Turk workerIDs have MORE THAN ONE subno#
participantNos <- aggregate(mydata$trueSubNo, by=list(mydata$WorkerId, mydata$trueSubNo), unique)#
participantNos <- participantNos[,1:2]#
names(participantNos) <- c('WorkerId','trueSubNo')#
#
#Make sure it's in the correct order!!#
participantNos <- participantNos[order(participantNos$trueSubNo),]#
participantNos$isDup <- duplicated(participantNos$WorkerId)#
#
#And merge back in#
mydata <- merge(mydata, participantNos, by=c('WorkerId','trueSubNo'), all.x=TRUE)#
#((if you were willow-only, you pass this part!))#
mydata[is.na(mydata$HasTurk),]$isDup <- FALSE#
##################################################################
## Recode and clean data & conditions#
#
#Check for number of legal responses given in a single session!!#
mydata$GotAnswer <- 0#
mydata[is.na(mydata$wasError),]$wasError <- '1' #Fix up missing values#
#
mydata[mydata$Response == 'Yes' & mydata$wasError == '0',]$GotAnswer <- 1#
mydata[mydata$Response == 'No' & mydata$wasError == '0',]$GotAnswer <- 1#
participant.responsecount <- aggregate(mydata$GotAnswer, by=list(mydata$Paycode), sum)#
names(participant.responsecount) <- c("Paycode", "LegalAnswers")#
mydata <- merge(mydata, participant.responsecount, by=c("Paycode"), all.x=TRUE)#
#Drop for analysis#
#(Remember, Willow-onliers got a free pass on the first 3 here...)#
#
length(unique(mydata$Paycode)) #214#
mydata <- mydata[mydata$Answer.country == "USA" &#
		mydata$Answer.English == "yes" &#
		mydata$LegalAnswers > 65 & #You did at least 3/4 of the trials#
		mydata$VideoProblem == FALSE & #No reported lag/issue#
		mydata$isDup == FALSE,] #You didn't take it before!!#
#Throw out people who only had turk data (no code given...)#
mydata <- mydata[!is.na(mydata$HasWillow),]#
#
length(unique(mydata$Paycode)) #105#
##################################################################
## CREATING DATA SUBSETS HERE#
#
mydata$wasCorrect <- as.numeric(as.character(mydata$wasCorrect))#
alldata <- mydata #
#
#What about people who didn't just ceiling out of the task?#
personScores <- aggregate(alldata$wasCorrect, by=list(alldata$Paycode), mean.na.rm)#
names(personScores) <-c("Paycode","overallScore")#
alldata <- merge(alldata, personScores)#
#
lowdata <- alldata[alldata$overallScore < 0.75,]#
#
#What about people who paid more/less attention to the flanking memory task?#
responseAScores <- aggregate(as.numeric(as.character(alldata$wordResponseA)), by=list(alldata$Paycode), mean.na.rm)#
names(responseAScores) <-c("Paycode","AScore")#
responseBScores <- aggregate(as.numeric(as.character(alldata$wordResponseB)), by=list(alldata$Paycode), mean.na.rm)#
names(responseBScores) <-c("Paycode","BScore")#
wordResponses <- merge(responseAScores, responseBScores, by=c("Paycode"))#
wordResponses$wordResponse <- wordResponses$AScore + wordResponses$BScore#
alldata <- merge(alldata, wordResponses)#
#
worddata <- alldata[alldata$wordResponse > 8,]#
middata <- alldata[alldata$wordResponse < 8,]#
#
#And look at the 1st and 2nd halves of the data!#
alldata$trialNo <- as.numeric(as.character(alldata$trialNo))#
firstdata <- alldata[alldata$trialNo < 37 ,]#
lastdata <- alldata[alldata$trialNo > 18,]#
#
##################################################################
## CREATE SCORE AND TABULATE DESCRIPTIVES#
#
#Calculate SCORE per person per condition#
Scores <- aggregate(alldata$wasCorrect, by=list(alldata$Paycode, alldata$sentType, alldata$sentOrder), sum)#
names(Scores) <- c("Paycode", "sentType",  "sentOrder", "CorrectScore")#
alldata <- merge(alldata, Scores, by=c("Paycode", "sentType", "sentOrder"))#
#
#TrialScores#
TrialScores <- aggregate(alldata$wasCorrect, by=list(alldata$trialNo, alldata$sentType, alldata$sentOrder), mean.na.rm)#
names(TrialScores) <- c("Paycode", "sentType",  "sentOrder", "CorrectScore")#
#Calculate SCORE per person per condition - LOWDATA#
lowScores <- aggregate(lowdata$wasCorrect, by=list(lowdata$Paycode, lowdata$sentType, lowdata$sentOrder), sum)#
names(lowScores) <- c("Paycode", "sentType",  "sentOrder", "CorrectScore")#
lowdata <- merge(lowdata, lowScores, by=c("Paycode", "sentType",  "sentOrder"))#
#
#Calculate SCORE per person per condition - WORDDATA#
wordScores <- aggregate(worddata$wasCorrect, by=list(worddata$Paycode, worddata$sentType, worddata$sentOrder), sum)#
names(wordScores) <- c("Paycode", "sentType",  "sentOrder", "CorrectScore")#
worddata <- merge(worddata, wordScores, by=c("Paycode", "sentType",  "sentOrder"))#
#
midScores <- aggregate(middata$wasCorrect, by=list(middata$Paycode, middata$sentType, middata$sentOrder), sum)#
names(midScores) <- c("Paycode", "sentType",  "sentOrder", "CorrectScore")#
middata <- merge(middata, midScores, by=c("Paycode", "sentType",  "sentOrder"))#
#
#Calculate SCORE per person per condition - first/last data#
firstScores <- aggregate(firstdata$wasCorrect, by=list(firstdata$Paycode, firstdata$sentType, firstdata$sentOrder), sum)#
names(firstScores) <- c("Paycode", "sentType",  "sentOrder", "CorrectScore")#
firstdata <- merge(firstdata, firstScores, by=c("Paycode", "sentType",  "sentOrder"))#
#
lastScores <- aggregate(lastdata$wasCorrect, by=list(lastdata$Paycode, lastdata$sentType, lastdata$sentOrder), sum)#
names(lastScores) <- c("Paycode", "sentType",  "sentOrder", "CorrectScore")#
lastdata <- merge(lastdata, lastScores, by=c("Paycode", "sentType",  "sentOrder"))#
#
##################################################################
## REPORT DESCRIPTIVES
Try again removing "Same" items#
diffdata <- alldata[alldata$changeType !="Same",]#
#Calculate SCORE per person per condition#
diffScores <- aggregate(diffdata$wasCorrect, by=list(diffdata$Paycode, diffdata$sentType, diffdata$sentOrder), sum)#
names(diffScores) <- c("Paycode", "sentType",  "sentOrder", "CorrectScore")#
diffdata <- merge(diffdata, Scores, by=c("Paycode", "sentType", "sentOrder"))#
#
with(diffScores, tapply(CorrectScore, list(sentType, sentOrder), mean, na.rm=TRUE), drop=TRUE)#
with(diffScores, tapply(CorrectScore, list(sentType, sentOrder), stderr), drop=TRUE)
revdata <- alldata[alldata$changeType == "Reversal",]#
#Calculate SCORE per person per condition#
revScores <- aggregate(revdata$wasCorrect, by=list(revdata$Paycode, revdata$sentType, revdata$sentOrder), sum)#
names(revScores) <- c("Paycode", "sentType",  "sentOrder", "CorrectScore")#
revdata <- merge(revdata, Scores, by=c("Paycode", "sentType", "sentOrder"))#
#
with(revScores, tapply(CorrectScore, list(sentType, sentOrder), mean, na.rm=TRUE), drop=TRUE)#
with(revScores, tapply(CorrectScore, list(sentType, sentOrder), stderr), drop=TRUE)
names(alldata)
alldata$questionType
names(alldata)
alldata$testSequence
passdata <- alldata[alldata$questionType == "Passive",]#
#Calculate SCORE per person per condition#
passScores <- aggregate(passdata$wasCorrect, by=list(passdata$Paycode, passdata$sentType, passdata$sentOrder), sum)#
names(passScores) <- c("Paycode", "sentType",  "sentOrder", "CorrectScore")#
passdata <- merge(passdata, Scores, by=c("Paycode", "sentType", "sentOrder"))#
#
with(passScores, tapply(CorrectScore, list(sentType, sentOrder), mean, na.rm=TRUE), drop=TRUE)#
with(passScores, tapply(CorrectScore, list(sentType, sentOrder), stderr), drop=TRUE)
alldata <- lastdata#
#Try again removing "Same" items#
diffdata <- alldata[alldata$changeType !="Same",]#
#Calculate SCORE per person per condition#
diffScores <- aggregate(diffdata$wasCorrect, by=list(diffdata$Paycode, diffdata$sentType, diffdata$sentOrder), sum)#
names(diffScores) <- c("Paycode", "sentType",  "sentOrder", "CorrectScore")#
diffdata <- merge(diffdata, Scores, by=c("Paycode", "sentType", "sentOrder"))#
#
with(diffScores, tapply(CorrectScore, list(sentType, sentOrder), mean, na.rm=TRUE), drop=TRUE)#
with(diffScores, tapply(CorrectScore, list(sentType, sentOrder), stderr), drop=TRUE)
revdata <- alldata[alldata$changeType == "Reversal",]#
#Calculate SCORE per person per condition#
revScores <- aggregate(revdata$wasCorrect, by=list(revdata$Paycode, revdata$sentType, revdata$sentOrder), sum)#
names(revScores) <- c("Paycode", "sentType",  "sentOrder", "CorrectScore")#
revdata <- merge(revdata, Scores, by=c("Paycode", "sentType", "sentOrder"))#
#
with(revScores, tapply(CorrectScore, list(sentType, sentOrder), mean, na.rm=TRUE), drop=TRUE)#
with(revScores, tapply(CorrectScore, list(sentType, sentOrder), stderr), drop=TRUE)
Try just Passive questions!#
#
passdata <- alldata[alldata$questionType == "Passive",]#
#Calculate SCORE per person per condition#
passScores <- aggregate(passdata$wasCorrect, by=list(passdata$Paycode, passdata$sentType, passdata$sentOrder), sum)#
names(passScores) <- c("Paycode", "sentType",  "sentOrder", "CorrectScore")#
passdata <- merge(passdata, Scores, by=c("Paycode", "sentType", "sentOrder"))#
#
with(passScores, tapply(CorrectScore, list(sentType, sentOrder), mean, na.rm=TRUE), drop=TRUE)#
with(passScores, tapply(CorrectScore, list(sentType, sentOrder), stderr), drop=TRUE)
passTransScores <- passScores[passScores$sentType == "AO" | passScores$sentType == "IO",]#
fit <- lm(CorrectScore ~ sentType * sentOrder + (1/Paycode), passTransScores)#
anova(fit)
